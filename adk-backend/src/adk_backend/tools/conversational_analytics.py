from __future__ import annotations

import json
import logging
from typing import Any, Dict, List, Optional

from google.cloud import geminidataanalytics
from google.cloud import datacatalog_v1
from google.auth import default

from ..config import get_settings
from .reasoning_tracker import ReasoningTracker

_settings = get_settings()
DEFAULT_PROJECT_ID = _settings.google_project_id
DEFAULT_DATASET_ID = (
    _settings.bigquery_default_dataset
    or _settings.bigquery_dataset
    or "divorce_analytics"
)
logger = logging.getLogger(__name__)


# @tool ë°ì½”ë ˆì´í„° ì¬ì‚¬ìš© (bigquery.pyì™€ ë™ì¼)
def tool(description: str = "", **kwargs):
    """Google ADK FunctionToolì„ ìœ„í•œ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        func._is_adk_tool = True
        func._tool_description = description
        func._tool_kwargs = kwargs
        return func
    return decorator


@tool(
    name="conversational.ask_insights",
    description="ìì—°ì–´ ì§ˆë¬¸ìœ¼ë¡œ BigQuery ë°ì´í„° ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
)
def ask_data_insights(
    question: str,
    table_names: Optional[str] = None,
) -> str:
    """
    ìì—°ì–´ ì§ˆë¬¸ìœ¼ë¡œ BigQuery ë°ì´í„° ì¸ì‚¬ì´íŠ¸ ìƒì„±.

    Args:
        question: ìì—°ì–´ ì§ˆë¬¸ (ì˜ˆ: "ì§€ë‚œ 7ì¼ê°„ ê°€ì¥ ë§ì€ ìœ„í˜‘ì€?")
        table_names: ë¶„ì„í•  í…Œì´ë¸” ëª©ë¡ (ì‰¼í‘œ êµ¬ë¶„, optional)

    Returns:
        JSON ë¬¸ìì—´ í˜•íƒœì˜ AI ìƒì„± ì¸ì‚¬ì´íŠ¸
    """
    try:
        # ğŸš¨ í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • í™•ì¸
        import os
        logger.info("=" * 80)
        logger.info("ğŸ”§ Conversational Analytics - í™˜ê²½ ì„¤ì • í™•ì¸")
        logger.info("=" * 80)
        logger.info(f"GOOGLE_APPLICATION_CREDENTIALS: {os.getenv('GOOGLE_APPLICATION_CREDENTIALS')}")
        logger.info(f"GOOGLE_CLOUD_PROJECT: {os.getenv('GOOGLE_CLOUD_PROJECT')}")
        logger.info(f"Settings - google_project_id: {_settings.google_project_id}")
        logger.info(f"Settings - bigquery_default_dataset: {_settings.bigquery_default_dataset}")
        logger.info(f"Settings - bigquery_location: {getattr(_settings, 'bigquery_location', 'N/A')}")
        logger.info("=" * 80)

        # ğŸ§  ì¶”ë¡  ì¶”ì  ì‹œì‘
        tracker = ReasoningTracker()

        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦
        credentials, _ = default()
        logger.info(f"âœ… ì¸ì¦ ì„±ê³µ: {type(credentials).__name__}")

        # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = geminidataanalytics.DataChatServiceClient(
            credentials=credentials
        )

        # í”„ë¡œì íŠ¸ ì •ë³´
        project_id = DEFAULT_PROJECT_ID
        dataset_id = DEFAULT_DATASET_ID

        # 1ë‹¨ê³„: ì§ˆë¬¸ ë¶„ì„
        intent = _analyze_question_intent(question)
        required_data_types = _identify_required_data(question)

        tracker.add_question_analysis(
            question=question,
            intent=intent,
            required_data=required_data_types
        )

        # í…Œì´ë¸” ëª©ë¡ íŒŒì‹± ë° ì„ ì •
        if not table_names:
            # ê¸°ë³¸ í…Œì´ë¸”ë“¤ (ì´í˜¼ íŒë¡€ ë¶„ì„ ì „ìš©)
            all_available_tables = [
                "precedent_cases",  # âœ… í•µì‹¬ íŒë¡€ ë°ì´í„° (ìœ„ìë£Œ, ì¬ì‚°ë¶„í•  ë“±)
                "divorce_case_metadata",
            ]
            tables, table_reasons = _select_relevant_tables(question, intent, all_available_tables)
        else:
            tables = [t.strip() for t in table_names.split(",")]
            table_reasons = {t: "ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •" for t in tables}

        # 2ë‹¨ê³„: í…Œì´ë¸” ì„ ì • ì¶”ë¡ 
        tracker.add_table_selection(
            selected_tables=tables,
            reasons=table_reasons,
            alternatives_considered=None
        )

        # BigQuery í…Œì´ë¸” ì°¸ì¡° ìƒì„±
        bq_table_references = []
        def _resolve_table_reference(table_name: str) -> geminidataanalytics.BigQueryTableReference:
            parts = table_name.split(".")
            if len(parts) == 3:
                proj, dataset, table = parts
            elif len(parts) == 2:
                proj = project_id
                dataset, table = parts
            else:
                proj = project_id
                dataset = dataset_id
                table = parts[0]

            if not dataset:
                dataset = DEFAULT_DATASET_ID

            return geminidataanalytics.BigQueryTableReference(
                project_id=proj,
                dataset_id=dataset,
                table_id=table,
            )

        for table in tables:
            ref = _resolve_table_reference(table)
            bq_table_references.append(ref)

        # ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •
        bq_refs = geminidataanalytics.BigQueryTableReferences(
            table_references=bq_table_references
        )
        datasource_refs = geminidataanalytics.DatasourceReferences(bq=bq_refs)

        # ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ (stateless ëŒ€í™”ìš©)
        context = geminidataanalytics.Context(
            datasource_references=datasource_refs
        )

        # ë©”ì‹œì§€ ìƒì„±
        user_message = geminidataanalytics.UserMessage(text=question)
        message = geminidataanalytics.Message(user_message=user_message)

        # ìš”ì²­ ìƒì„±
        request = geminidataanalytics.ChatRequest(
            parent=f"projects/{project_id}/locations/global",
            messages=[message],
            inline_context=context,
        )

        # 3ë‹¨ê³„: ì¿¼ë¦¬ ì „ëµ ì¶”ë¡ 
        strategy_type, operations = _infer_query_strategy(question, intent)
        tracker.add_query_strategy(
            strategy_type=strategy_type,
            operations=operations,
            rationale=f"{intent}ë¥¼ ìœ„í•´ {strategy_type} ì „ëµ ì‚¬ìš©"
        )

        # ğŸš¨ ì‹¤ì œ API í˜¸ì¶œ í™•ì¸ìš© ë¡œê·¸
        logger.info("=" * 60)
        logger.info("ğŸ”¥ REAL API CALL - Google Gemini Data Analytics API")
        logger.info("=" * 60)
        logger.info(f"Question: {question}")
        logger.info(f"Tables: {tables}")
        logger.info(f"Project: {project_id}")
        logger.info(f"Dataset: {dataset_id}")
        logger.info("API í˜¸ì¶œ ì‹œì‘... (ì•µë¬´ìƒˆ ì•„ë‹˜!)")

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
        response_text = ""
        generated_sql = None

        stream = client.chat(request=request)
        logger.info("ğŸ“¡ Streaming response from Google API...")
        for response in stream:
            if hasattr(response, 'agent_message') and response.agent_message.text:
                response_text += response.agent_message.text

            # SQL ì¿¼ë¦¬ ì¶”ì¶œ (ìˆë‹¤ë©´)
            if hasattr(response, 'agent_message') and hasattr(response.agent_message, 'generated_sql'):
                generated_sql = response.agent_message.generated_sql

        # ğŸš¨ API ì‘ë‹µ í™•ì¸ìš© ë¡œê·¸
        logger.info("=" * 60)
        logger.info("âœ… API Response Received")
        logger.info("=" * 60)
        logger.info(f"Response length: {len(response_text)} characters")
        logger.info(f"Has SQL: {generated_sql is not None}")
        if generated_sql:
            logger.info(f"SQL preview: {generated_sql[:200]}...")
        logger.info(f"Response preview: {response_text[:300]}...")
        logger.info("=" * 60)

        # 4ë‹¨ê³„: ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ì¶”ë¡ 
        if response_text:
            findings_preview = response_text[:200] + "..." if len(response_text) > 200 else response_text
            tracker.add_insight_derivation(
                findings=findings_preview,
                interpretation="Google Gemini APIê°€ ë°ì´í„° ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ",
                confidence=0.9
            )

        # ì¶”ë¡  ê²°ê³¼ í¬ë§·íŒ…
        formatted_reasoning = tracker.get_formatted_reasoning()
        reasoning_summary = tracker.get_summary_list()
        reasoning_detail = tracker.to_dict()

        logger.info(f"ğŸ§  Reasoning steps generated: {len(tracker.steps)} steps")

        result = {
            "question": question,
            "tables_analyzed": tables,
            "insight": response_text,
            "generated_sql": generated_sql,
            "reasoning": reasoning_summary,
            "reasoning_detail": reasoning_detail,
            "reasoning_formatted": formatted_reasoning,
            "analysis_method": "conversational_analytics_api_v2",
        }

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        logger.error("=" * 60)
        logger.error("âŒ API Call Failed")
        logger.error("=" * 60)
        logger.error(f"Error type: {type(e).__name__}")
        logger.error(f"Error message: {str(e)}")
        logger.error("=" * 60)

        error_result = {
            "error": str(e),
            "error_type": type(e).__name__,
            "question": question,
            "is_real_api_error": True,  # ì‹¤ì œ API í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@tool(
    name="conversational.search_catalog",
    description="BigQuery ì¹´íƒˆë¡œê·¸ì—ì„œ í…Œì´ë¸”/ë·°/ëª¨ë¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."
)
def search_catalog(
    query: str,
    max_results: Optional[int] = 20,
) -> str:
    """
    BigQuery ì¹´íƒˆë¡œê·¸ì—ì„œ í…Œì´ë¸”/ë·°/ëª¨ë¸ ê²€ìƒ‰.

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: "ë³´ì•ˆ", "ì „í™˜")
        max_results: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ê°’: 20)

    Returns:
        JSON ë¬¸ìì—´ í˜•íƒœì˜ ê²€ìƒ‰ëœ ë¦¬ì†ŒìŠ¤ ëª©ë¡
    """
    try:
        credentials, _ = default()
        client = datacatalog_v1.DataCatalogClient(credentials=credentials)

        project_id = _settings.google_project_id

        # ê²€ìƒ‰ ë²”ìœ„ ì„¤ì •
        scope = datacatalog_v1.SearchCatalogRequest.Scope(
            include_project_ids=[project_id]
        )

        # ê²€ìƒ‰ ì‹¤í–‰
        search_request = datacatalog_v1.SearchCatalogRequest(
            scope=scope,
            query=query,
            page_size=max_results,
        )

        results = []
        for result in client.search_catalog(request=search_request):
            results.append({
                "name": result.relative_resource_name,
                "type": result.search_result_type.name,
                "linked_resource": result.linked_resource if hasattr(result, "linked_resource") else "",
            })

        catalog_result = {
            "query": query,
            "result_count": len(results),
            "resources": results,
        }

        return json.dumps(catalog_result, ensure_ascii=False, indent=2)

    except Exception as e:
        error_result = {
            "error": str(e),
            "query": query,
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)


# ğŸ§  ì¶”ë¡  í—¬í¼ í•¨ìˆ˜ë“¤
def _analyze_question_intent(question: str) -> str:
    """ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„"""
    question_lower = question.lower()

    # ì˜ë„ íŒ¨í„´ ë§¤ì¹­
    if any(keyword in question_lower for keyword in ["ê±´ìˆ˜", "ìˆ˜", "ê°œìˆ˜", "ëª‡", "ì–¼ë§ˆë‚˜"]):
        return "ë°ì´í„° ì§‘ê³„ ë° ì¹´ìš´íŒ…"
    elif any(keyword in question_lower for keyword in ["ê°€ì¥", "ìµœê³ ", "ìµœëŒ€", "ìµœì†Œ", "top"]):
        return "ìˆœìœ„ ë° ê·¹ê°’ ë¶„ì„"
    elif any(keyword in question_lower for keyword in ["ë¹„êµ", "ì°¨ì´", "vs", "ëŒ€ë¹„"]):
        return "ë¹„êµ ë¶„ì„"
    elif any(keyword in question_lower for keyword in ["ì¶”ì„¸", "ë³€í™”", "íŠ¸ë Œë“œ", "ì¶”ì´"]):
        return "ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„"
    elif any(keyword in question_lower for keyword in ["ì „í™˜", "ì „í™˜ìœ¨", "conversion"]):
        return "ì „í™˜ìœ¨ ë° í¼ë„ ë¶„ì„"
    elif any(keyword in question_lower for keyword in ["í‰ê· ", "ì¤‘ì•™ê°’", "ë¶„í¬"]):
        return "í†µê³„ ë¶„ì„"
    else:
        return "ì¼ë°˜ ë°ì´í„° ì¡°íšŒ"


def _identify_required_data(question: str) -> List[str]:
    """ì§ˆë¬¸ì— í•„ìš”í•œ ë°ì´í„° íƒ€ì… ì‹ë³„"""
    question_lower = question.lower()
    required_data = []

    # ë„ë©”ì¸ë³„ í‚¤ì›Œë“œ ë§¤í•‘ (ì´í˜¼ íŒë¡€ ë„ë©”ì¸)
    if any(keyword in question_lower for keyword in ["ìœ„ìë£Œ", "alimony", "ê¸ˆì•¡", "ë³´ìƒ"]):
        required_data.append("ìœ„ìë£Œ í†µê³„ ë°ì´í„° (precedent_cases.alimony_amount)")
    if any(keyword in question_lower for keyword in ["ì¬ì‚°", "ë¶„í• ", "property", "ratio", "ë¹„ìœ¨"]):
        required_data.append("ì¬ì‚°ë¶„í•  í†µê³„ ë°ì´í„° (precedent_cases.property_ratio_plaintiff)")
    if any(keyword in question_lower for keyword in ["ìœ ì±…", "ì‚¬ìœ ", "fault", "ë¶€ì •í–‰ìœ„", "ì™¸ë„", "í­ì–¸"]):
        required_data.append("ì´í˜¼ ì‚¬ìœ ë³„ íŒë¡€ ë°ì´í„° (precedent_cases.fault_type)")
    if any(keyword in question_lower for keyword in ["íƒœê·¸", "ê²€ìƒ‰", "tags"]):
        required_data.append("íŒë¡€ ê²€ìƒ‰ íƒœê·¸ (precedent_cases.tags)")

    # ì‹œê°„ ê´€ë ¨
    if any(keyword in question_lower for keyword in ["ì¼", "ì£¼", "ì›”", "ë…„", "ê¸°ê°„", "ë‚ ì§œ", "ì„ ê³ "]):
        required_data.append("ì„ ê³  ì‹œì  ì •ë³´ (judgment_date)")

    return required_data if required_data else ["ì¼ë°˜ íŒë¡€ ë°ì´í„°"]


def _select_relevant_tables(
    question: str,
    intent: str,
    available_tables: List[str]
) -> tuple[List[str], Dict[str, str]]:
    """ì§ˆë¬¸ê³¼ ì˜ë„ì— ê¸°ë°˜í•˜ì—¬ ê´€ë ¨ í…Œì´ë¸” ì„ íƒ"""
    question_lower = question.lower()
    selected = []
    reasons = {}

    # í…Œì´ë¸”ë³„ í‚¤ì›Œë“œ ë§¤í•‘ (ì´í˜¼ íŒë¡€ ë„ë©”ì¸)
    table_keywords = {
        "precedent_cases": {
            "keywords": ["íŒë¡€", "ìœ„ìë£Œ", "ì¬ì‚°", "ë¶„í• ", "ìœ ì±…", "ì‚¬ìœ ", "ë¶€ì •í–‰ìœ„", "ê¸ˆì•¡", "ë¹„ìœ¨", "ê±´ìˆ˜", "í†µê³„"],
            "reason": "í•µì‹¬ ì´í˜¼ íŒë¡€ ë°ì´í„° (ìœ„ìë£Œ, ì¬ì‚°ë¶„í• ë¹„ìœ¨, ìœ ì±…ì‚¬ìœ  ë“± í¬í•¨)"
        },
        "divorce_case_metadata": {
            "keywords": ["ë©”íƒ€", "ë°ì´í„°", "ê´€ë¦¬"],
            "reason": "íŒë¡€ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì •ë³´"
        }
    }

    # í‚¤ì›Œë“œ ê¸°ë°˜ í…Œì´ë¸” ì„ íƒ
    for table in available_tables:
        if table in table_keywords:
            config = table_keywords[table]
            if any(keyword in question_lower for keyword in config["keywords"]):
                selected.append(table)
                reasons[table] = config["reason"]

    # ì„ íƒëœ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ í…Œì´ë¸” ì‚¬ìš©
    if not selected:
        selected = ["precedent_cases"]
        reasons = {
            "precedent_cases": "ê¸°ë³¸ ì´í˜¼ íŒë¡€ í†µê³„ ë°ì´í„°"
        }

    return selected, reasons


def _infer_query_strategy(question: str, intent: str) -> tuple[str, List[str]]:
    """ì§ˆë¬¸ê³¼ ì˜ë„ì—ì„œ ì¿¼ë¦¬ ì „ëµ ì¶”ë¡ """
    question_lower = question.lower()
    operations = []

    # ì˜ë„ë³„ ì „ëµ
    if "ì§‘ê³„" in intent or "ì¹´ìš´íŒ…" in intent:
        strategy = "ì§‘ê³„ ì¿¼ë¦¬ (Aggregation)"
        operations = ["COUNT() í•¨ìˆ˜ë¡œ ê±´ìˆ˜ ì§‘ê³„"]
    elif "ìˆœìœ„" in intent or "ê·¹ê°’" in intent:
        strategy = "ìˆœìœ„ ì¿¼ë¦¬ (Ranking)"
        operations = ["ORDER BYë¡œ ì •ë ¬", "LIMITë¡œ ìƒìœ„/í•˜ìœ„ ì¶”ì¶œ"]
    elif "ë¹„êµ" in intent:
        strategy = "ë¹„êµ ì¿¼ë¦¬ (Comparison)"
        operations = ["GROUP BYë¡œ ê·¸ë£¹í™”", "ë¹„êµ ë©”íŠ¸ë¦­ ê³„ì‚°"]
    elif "íŠ¸ë Œë“œ" in intent:
        strategy = "ì‹œê³„ì—´ ì¿¼ë¦¬ (Time Series)"
        operations = ["ë‚ ì§œë³„ GROUP BY", "ì‹œê°„ ìˆœì„œ ì •ë ¬"]
    elif "ì „í™˜" in intent:
        strategy = "í¼ë„ ë¶„ì„ (Funnel Analysis)"
        operations = ["ì´ë²¤íŠ¸ ì¡°ì¸", "ì „í™˜ìœ¨ ê³„ì‚°"]
    elif "í†µê³„" in intent:
        strategy = "í†µê³„ ì¿¼ë¦¬ (Statistical)"
        operations = ["AVG(), MEDIAN() ë“± í†µê³„ í•¨ìˆ˜", "ë¶„í¬ ê³„ì‚°"]
    else:
        strategy = "ì¼ë°˜ SELECT ì¿¼ë¦¬"
        operations = ["ê¸°ë³¸ ë°ì´í„° ì¡°íšŒ"]

    # ì‹œê°„ í•„í„°ë§ ê°ì§€
    if any(keyword in question_lower for keyword in ["ì§€ë‚œ", "ìµœê·¼", "ì´ë²ˆ", "ì‘ë…„"]):
        operations.append("ë‚ ì§œ í•„í„°ë§ (WHERE date >= ...)")

    # ì¡°ê±´ í•„í„°ë§ ê°ì§€
    if any(keyword in question_lower for keyword in ["ë“±ê¸‰", "íƒ€ì…", "ìœ í˜•", "ì¢…ë¥˜"]):
        operations.append("ì¡°ê±´ í•„í„°ë§ (WHERE type = ...)")

    return strategy, operations
