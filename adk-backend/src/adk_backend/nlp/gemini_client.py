import os
import json
from typing import Dict, List, Optional, Any
import logging
import google.generativeai as genai
from google.cloud import bigquery
from .prompt_templates import PromptType, PromptTemplate

logger = logging.getLogger(__name__)

class GeminiClient:
    """Google Gemini API í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤"""

    def __init__(self, api_key: Optional[str] = None, model: str = "gemini-2.5-flash", cag_metadata: Optional[str] = None, enable_cag_caching: bool = False, cag_cache_ttl_minutes: int = 60):
        """
        Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

        Args:
            api_key: Google API í‚¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ)
            model: ì‚¬ìš©í•  ëª¨ë¸ëª…
            cag_metadata: Context Cacheìš© íŒë¡€ ë©”íƒ€ë°ì´í„° JSON (middlewareì—ì„œ ì£¼ì…)
            enable_cag_caching: ì•”ì‹œì  ìºì‹± í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            cag_cache_ttl_minutes: CAG ìºì‹œ TTL in minutes (ê¸°ë³¸ê°’: 60)
        """
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        self.model_name = model
        self.cag_metadata = cag_metadata if enable_cag_caching else None  # ìºì‹± ë¹„í™œì„±í™” ì‹œ ë©”íƒ€ë°ì´í„° ë¯¸ì£¼ì…
        self.enable_cag_caching = enable_cag_caching  # ì•”ì‹œì  ìºì‹± í™œì„±í™” ì—¬ë¶€
        self.cag_cache_ttl_minutes = cag_cache_ttl_minutes  # ìºì‹œ TTL (ë¶„)

        if not self.api_key:
            logger.warning("Google API key not found. Some features will be limited.")
            self.model = None
        else:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel(self.model_name)

        self.prompt_templates = self._load_prompt_templates()
        self.bq_client = bigquery.Client() if self._should_init_bq() else None

    def _should_init_bq(self) -> bool:
        """BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            # ê°„ë‹¨í•œ test: BigQuery í´ë¼ì´ì–¸íŠ¸ ìƒì„± ê°€ëŠ¥í•œì§€ í™•ì¸
            client = bigquery.Client()
            client.get_dataset("divorce_analytics")
            return True
        except Exception as e:
            logger.warning(f"BigQuery client initialization failed: {e}")
            return False


    def _load_prompt_templates(self) -> Dict[PromptType, PromptTemplate]:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ"""
        templates = {
            PromptType.INTENT_ANALYSIS: PromptTemplate(
                name="ì´í˜¼ ìƒë‹´ ì˜ë„ ë¶„ì„",
                system_message="""
ë‹¹ì‹ ì€ ì´í˜¼ ì†Œì†¡ ê´€ë ¨ ìì—°ì–´ ì§ˆì˜ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ì˜ë„ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

- COUNT: íŒë¡€ ê°œìˆ˜ë¥¼ ì„¸ëŠ” ì§ˆë¬¸ (ì˜ˆ: 'ë¶€ì •í–‰ìœ„ íŒë¡€ê°€ ëª‡ ê°œì¸ê°€ìš”?')
- AGGREGATE: ìœ„ìë£Œ í‰ê· , ì¬ì‚°ë¶„í•  ë¹„ìœ¨ ë“± ì§‘ê³„ (ì˜ˆ: 'í‰ê·  ìœ„ìë£ŒëŠ” ì–¼ë§ˆì¸ê°€ìš”?')
- FILTER: íŠ¹ì • ì¡°ê±´(ìœ ì±… ì‚¬ìœ  ë“±)ìœ¼ë¡œ ë°ì´í„° í•„í„°ë§
- TREND: ì‹œê¸°ë³„ ì´í˜¼ ì¶”ì„¸ ë¶„ì„
- COMPARISON: ìœ ì±… ì‚¬ìœ ë³„ ìœ„ìë£Œ ë¹„êµ ë˜ëŠ” ê¸°ê°„ ë¹„êµ
- SEARCH: íŠ¹ì • í‚¤ì›Œë“œë‚˜ íƒœê·¸ê°€ í¬í•¨ëœ íŒë¡€ ê²€ìƒ‰
- RANKING: ìœ„ìë£Œ ì•¡ìˆ˜ ê¸°ì¤€ ìƒìœ„ íŒë¡€ ë“±
- DISTRIBUTION: ìœ„ìë£Œ ì•¡ìˆ˜ëŒ€ë³„ ë¶„í¬ ë¶„ì„

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ í•´ì£¼ì„¸ìš”: {"intent": "INTENT_NAME", "confidence": 0.95}
""",
                user_template="ë‹¤ìŒ ì´í˜¼ ê´€ë ¨ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”: {query}",
                examples=[
                    {
                        "input": "ë¶€ì •í–‰ìœ„ë¡œ ì¸í•œ ìœ„ìë£Œ í‰ê· ì´ ì–¼ë§ˆì¸ê°€ìš”?",
                        "output": '{"intent": "AGGREGATE", "confidence": 0.98}'
                    },
                    {
                        "input": "ìµœê·¼ 1ë…„ê°„ í­ì–¸ ê´€ë ¨ íŒë¡€ë¥¼ ëª¨ë‘ ì°¾ì•„ì£¼ì„¸ìš”",
                        "output": '{"intent": "FILTER", "confidence": 0.95}'
                    }
                ],
                max_tokens=100,
                temperature=0.1
            ),
            
            PromptType.ENTITY_EXTRACTION: PromptTemplate(
                name="ì´í˜¼ ìƒë‹´ ì—”í‹°í‹° ì¶”ì¶œ",
                system_message="""
ë‹¹ì‹ ì€ ì´í˜¼ ì†Œì†¡ ì§ˆì˜ì—ì„œ ë¶„ì„ì— í•„ìš”í•œ ì£¼ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

1. table: í•­ìƒ 'precedent_cases' ê³ ì •
2. fault_type: ìœ ì±… ì‚¬ìœ  (ì˜ˆ: 'ë¶€ì •í–‰ìœ„', 'í­ì–¸', 'ë„ë°•', 'ê³ ë¶€ê°ˆë“±' ë“±)
3. alimony_range: ìœ„ìë£Œ ë²”ìœ„ (ì˜ˆ: '2000ë§Œì› ì´ìƒ', '3000ë§Œì› ë¯¸ë§Œ')
4. property_ratio: ì¬ì‚°ë¶„í•  ë¹„ìœ¨ ê´€ë ¨ í‚¤ì›Œë“œ
5. time_period: ë¶„ì„ ê¸°ê°„ (ì˜ˆ: 'ìµœê·¼ 3ë…„', '2023ë…„ ì´í›„')
6. aggregation: ì‚¬ìš©í•  ì§‘ê³„ í•¨ìˆ˜ (COUNT, AVG_ALIMONY, AVG_PROPERTY_RATIO ë“±)

ì‘ë‹µ ì˜ˆì‹œ: {"table": "precedent_cases", "fault_type": "ë¶€ì •í–‰ìœ„", "aggregation": "AVG_ALIMONY"}
""",
                user_template="ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”: {query}",
                examples=[
                    {
                        "input": "ìƒê°„ë…€ ì†Œì†¡ ì‹œ ìœ„ìë£ŒëŠ” ë³´í†µ ì–¼ë§ˆë‚˜ ë‚˜ì˜¤ë‚˜ìš”?",
                        "output": '{"table": "precedent_cases", "fault_type": "ë¶€ì •í–‰ìœ„", "aggregation": "AVG_ALIMONY"}'
                    }
                ],
                max_tokens=200,
                temperature=0.1
            ),

            PromptType.RESULT_INTERPRETATION: PromptTemplate(
                name="ê²°ê³¼ í•´ì„",
                system_message="""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í•´ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
SQL ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ê·œì¹™:
- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…
- ìˆ«ìëŠ” ì²œ ë‹¨ìœ„ êµ¬ë¶„ì ì‚¬ìš©
- ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸ë‚˜ íŒ¨í„´ ê°•ì¡°
- ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±
""",
                user_template="ì§ˆë¬¸: {query}\nSQL: {sql}\nê²°ê³¼: {result}\n\nìœ„ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í•´ì„í•´ì£¼ì„¸ìš”.",
                examples=[],
                max_tokens=300,
                temperature=0.3
            ),
            
            PromptType.CHART_RECOMMENDATION: PromptTemplate(
                name="ì°¨íŠ¸ ì¶”ì²œ",
                system_message="""
ë‹¹ì‹ ì€ ë°ì´í„° ì‹œê°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¿¼ë¦¬ ê²°ê³¼ì— ê°€ì¥ ì í•©í•œ ì°¨íŠ¸ íƒ€ì…ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ì°¨íŠ¸ íƒ€ì…:
- bar: ì¹´í…Œê³ ë¦¬ë³„ ë¹„êµ
- line: ì‹œê°„ì— ë”°ë¥¸ ë³€í™”
- pie: ì „ì²´ì—ì„œ ê° ë¶€ë¶„ì˜ ë¹„ìœ¨
- table: ìƒì„¸ ë°ì´í„° í‘œì‹œ

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ í•´ì£¼ì„¸ìš”: {"chart_type": "bar", "reason": "ì´ìœ "}
""",
                user_template="ì§ˆë¬¸: {query}\nê²°ê³¼ ì»¬ëŸ¼: {columns}\në°ì´í„° íƒ€ì…: {data_types}\n\nê°€ì¥ ì í•©í•œ ì°¨íŠ¸ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
                examples=[],
                max_tokens=150,
                temperature=0.2
            )
        }
        
        return templates
    
    def is_available(self) -> bool:
        """Gemini API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return self.model is not None

    def get_caching_status(self) -> Dict[str, Any]:
        """
        CAG ì•”ì‹œì  ìºì‹± ìƒíƒœ ì¡°íšŒ (ëª¨ë‹ˆí„°ë§ìš©)

        Returns:
            ìºì‹± ì„¤ì • ë° ìƒíƒœ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        return {
            "caching_enabled": self.enable_cag_caching,
            "cache_ttl_minutes": self.cag_cache_ttl_minutes,
            "cag_metadata_loaded": self.cag_metadata is not None,
            "cag_metadata_size_bytes": len(self.cag_metadata) if self.cag_metadata else 0,
        }

    def _collect_response_text(self, response: Any, context: str) -> Optional[str]:
        """Gemini ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ"""
        if not response:
            logger.warning("Gemini response is empty for %s", context)
            return None

        # ë°©ë²• 1: response.text ì†ì„± ì‚¬ìš© (ê°€ì¥ ê°„ë‹¨)
        try:
            if hasattr(response, 'text'):
                text = response.text
                if text:
                    logger.info(f"[{context}] Successfully extracted text using response.text: {text[:100]}...")
                    return text.strip()
                else:
                    logger.warning(f"[{context}] response.text exists but is empty")
        except Exception as e:
            logger.warning(f"[{context}] Failed to access response.text: {e}")

        # ë°©ë²• 2: candidates êµ¬ì¡° ì‚¬ìš© (fallback)
        logger.info(f"[{context}] Trying fallback method via candidates")
        candidates = getattr(response, "candidates", None)
        if not candidates:
            logger.warning(f"[{context}] No candidates found. Response type: {type(response)}, dir: {dir(response)[:10]}...")
            return None

        logger.info(f"[{context}] Found {len(candidates)} candidate(s)")

        candidate = candidates[0]
        finish_reason = getattr(candidate, 'finish_reason', 'UNKNOWN')
        logger.info(f"[{context}] Finish reason: {finish_reason}")

        content = getattr(candidate, "content", None)
        if not content:
            logger.warning(f"[{context}] No content in candidate. Candidate type: {type(candidate)}, dir: {dir(candidate)[:10]}...")
            return None

        logger.info(f"[{context}] Content found: {type(content)}")

        parts = getattr(content, "parts", None)
        if parts is None:
            logger.warning(f"[{context}] Parts is None. Content type: {type(content)}, dir: {dir(content)[:10]}...")
            return None

        if not parts:  # Empty list
            logger.warning(f"[{context}] Parts list is empty (length: {len(parts)})")
            return None

        logger.info(f"[{context}] Found {len(parts)} part(s)")

        collected_parts: List[str] = []
        for i, part in enumerate(parts):
            text = getattr(part, "text", None)
            logger.info(f"[{context}] Part {i}: text length = {len(text) if text else 0}")
            if text:
                collected_parts.append(text)

        if not collected_parts:
            logger.warning(f"[{context}] No text found in any parts")
            return None

        result = "".join(collected_parts).strip()
        logger.info(f"[{context}] Successfully extracted {len(result)} characters")
        return result
    
    def _format_prompt_for_gemini(self, system_message: str, user_message: str, examples: List[Dict[str, str]] = None) -> str:
        """Geminiìš© í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
        prompt = f"{system_message}\n\n"
        
        if examples:
            prompt += "ì˜ˆì‹œ:\n"
            for example in examples:
                prompt += f"ì…ë ¥: {example['input']}\nì¶œë ¥: {example['output']}\n\n"
        
        prompt += f"ì§ˆë¬¸: {user_message}\në‹µë³€:"
        return prompt
    
    async def generate_completion(
        self, 
        prompt_type: PromptType, 
        **kwargs
    ) -> Optional[str]:
        """
        Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì™„ì„± ìƒì„±
        
        Args:
            prompt_type: í”„ë¡¬í”„íŠ¸ íƒ€ì…
            **kwargs: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì „ë‹¬í•  ë³€ìˆ˜ë“¤
            
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None (API ì‚¬ìš© ë¶ˆê°€ì‹œ)
        """
        if not self.is_available():
            logger.warning(f"Gemini API not available for {prompt_type.value}")
            return None
            
        try:
            template = self.prompt_templates[prompt_type]
            user_message = template.user_template.format(**kwargs)
            
            prompt = self._format_prompt_for_gemini(
                template.system_message, 
                user_message, 
                template.examples
            )
            
            generation_config = genai.types.GenerationConfig(
                max_output_tokens=template.max_tokens,
                temperature=template.temperature
            )

            # ì•ˆì „ ì„¤ì • ì™„í™” (ë°ì´í„° ë¶„ì„/ì¿¼ë¦¬ ìƒì„± ëª©ì )
            safety_settings = {
                genai.types.HarmCategory.HARM_CATEGORY_HATE_SPEECH: genai.types.HarmBlockThreshold.BLOCK_NONE,
                genai.types.HarmCategory.HARM_CATEGORY_HARASSMENT: genai.types.HarmBlockThreshold.BLOCK_NONE,
                genai.types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: genai.types.HarmBlockThreshold.BLOCK_NONE,
                genai.types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: genai.types.HarmBlockThreshold.BLOCK_NONE,
            }

            response = self.model.generate_content(
                prompt,
                generation_config=generation_config,
                safety_settings=safety_settings
            )

            result_text = self._collect_response_text(response, prompt_type.value)
            if not result_text:
                return None
            logger.info(f"Gemini API call successful for {prompt_type.value}")
            return result_text

        except Exception as e:
            logger.error(f"Gemini API call failed for {prompt_type.value}: {str(e)}")
            return None
    
    def interpret_result(
        self, 
        query: str, 
        sql: str, 
        result: List[Dict[str, Any]]
    ) -> Optional[str]:
        """ê²°ê³¼ í•´ì„ (ë™ê¸° ë²„ì „)"""
        if not self.is_available():
            return None
            
        try:
            template = self.prompt_templates[PromptType.RESULT_INTERPRETATION]
            serialized_result = json.dumps(result[:5], ensure_ascii=False)
            serialized_result = serialized_result.replace('{', '{{').replace('}', '}}')

            user_message = template.user_template.format(
                query=query,
                sql=sql,
                result=serialized_result  # ì²˜ìŒ 5ê°œ í–‰ë§Œ ì „ë‹¬
            )
            
            prompt = self._format_prompt_for_gemini(
                template.system_message, 
                user_message
            )
            
            generation_config = genai.types.GenerationConfig(
                max_output_tokens=template.max_tokens,
                temperature=template.temperature
            )

            # ì•ˆì „ ì„¤ì • ì™„í™” (ë°ì´í„° ë¶„ì„/ì¿¼ë¦¬ ìƒì„± ëª©ì )
            safety_settings = {
                genai.types.HarmCategory.HARM_CATEGORY_HATE_SPEECH: genai.types.HarmBlockThreshold.BLOCK_NONE,
                genai.types.HarmCategory.HARM_CATEGORY_HARASSMENT: genai.types.HarmBlockThreshold.BLOCK_NONE,
                genai.types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: genai.types.HarmBlockThreshold.BLOCK_NONE,
                genai.types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: genai.types.HarmBlockThreshold.BLOCK_NONE,
            }

            response = self.model.generate_content(
                prompt,
                generation_config=generation_config,
                safety_settings=safety_settings
            )
            
            return self._collect_response_text(response, "result_interpretation")
            
        except Exception as e:
            logger.error(f"Result interpretation failed: {str(e)}")
            return None
    
    def recommend_chart(
        self, 
        query: str, 
        columns: List[str], 
        data_types: List[str]
    ) -> Optional[Dict[str, str]]:
        """ì°¨íŠ¸ ì¶”ì²œ (ë™ê¸° ë²„ì „)"""
        if not self.is_available():
            return None
            
        try:
            template = self.prompt_templates[PromptType.CHART_RECOMMENDATION]
            user_message = template.user_template.format(
                query=query,
                columns=columns,
                data_types=data_types
            )
            
            prompt = self._format_prompt_for_gemini(
                template.system_message, 
                user_message
            )
            
            generation_config = genai.types.GenerationConfig(
                max_output_tokens=template.max_tokens,
                temperature=template.temperature
            )

            # ì•ˆì „ ì„¤ì • ì™„í™” (ë°ì´í„° ë¶„ì„/ì¿¼ë¦¬ ìƒì„± ëª©ì )
            safety_settings = {
                genai.types.HarmCategory.HARM_CATEGORY_HATE_SPEECH: genai.types.HarmBlockThreshold.BLOCK_NONE,
                genai.types.HarmCategory.HARM_CATEGORY_HARASSMENT: genai.types.HarmBlockThreshold.BLOCK_NONE,
                genai.types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: genai.types.HarmBlockThreshold.BLOCK_NONE,
                genai.types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: genai.types.HarmBlockThreshold.BLOCK_NONE,
            }

            response = self.model.generate_content(
                prompt,
                generation_config=generation_config,
                safety_settings=safety_settings
            )
            
            result = self._collect_response_text(response, "chart_recommendation")
            if not result:
                return None
            try:
                return json.loads(result)
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse JSON from Gemini response: {result}")
                return {"chart_type": "table", "reason": "ê¸°ë³¸ í…Œì´ë¸” í˜•ì‹"}
            
        except Exception as e:
            logger.error(f"Chart recommendation failed: {str(e)}")
            return None

    def get_embedding(self, text: str) -> Optional[List[float]]:
        """
        í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (text-embedding-004)
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë²¡í„° (float ë¦¬ìŠ¤íŠ¸) ë˜ëŠ” None
        """
        if not self.is_available():
            logger.warning("Gemini API not available for embeddings")
            return None

        try:
            # ì¤„ë°”ê¿ˆ ë“± ì „ì²˜ë¦¬
            text = text.replace("\n", " ")
            
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=text,
                task_type="retrieval_document",
                title="Embedding of court case"
            )
            
            if 'embedding' in result:
                return result['embedding']
            else:
                logger.warning("No embedding in result")
                return None
                
        except Exception as e:
            logger.error(f"Embedding generation failed: {str(e)}")
            return None

# ì „ì—­ Gemini í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ (lazy initialization with CAG)
_gemini_client_instance = None
_cag_metadata = None


def initialize_gemini_client_with_cag(cag_metadata: Optional[str] = None) -> GeminiClient:
    """
    CAG ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

    ì•”ì‹œì  ìºì‹±(Implicit Caching)ë§Œ ì‚¬ìš©:
    - ì €ì¥ ë¹„ìš© ì—†ìŒ (ë¦¬ìŠ¤í¬ ì œë¡œ)
    - ENABLE_CAG_CACHING í™˜ê²½ ë³€ìˆ˜ë¡œ í™œì„±í™”/ë¹„í™œì„±í™” ê°€ëŠ¥
    - ê¸°ë³¸ê°’: ë¹„í™œì„±í™” (False)

    Args:
        cag_metadata: CAG ë©”íƒ€ë°ì´í„° (ì—†ìœ¼ë©´ middlewareì—ì„œ ìë™ ë¡œë“œ)

    Returns:
        ì´ˆê¸°í™”ëœ GeminiClient ì¸ìŠ¤í„´ìŠ¤
    """
    global _gemini_client_instance, _cag_metadata
    from ..config import get_settings

    if _gemini_client_instance is not None:
        logger.warning("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return _gemini_client_instance

    # ì„¤ì •ì—ì„œ ìºì‹± í™œì„±í™” ì—¬ë¶€ ë° TTL ì½ê¸°
    settings = get_settings()
    enable_caching = settings.enable_cag_caching
    ttl_minutes = settings.cag_cache_ttl_minutes

    logger.info(f"ğŸ“¦ CAG ì•”ì‹œì  ìºì‹± ì„¤ì •: {'âœ… í™œì„±í™”' if enable_caching else 'âŒ ë¹„í™œì„±í™” (ì €ì¥ ë¹„ìš© ì—†ìŒ)'}")
    if enable_caching:
        logger.info(f"â±ï¸ CAG ìºì‹œ TTL: {ttl_minutes}ë¶„")

    # ìºì‹± í™œì„±í™” ì‹œì—ë§Œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
    if enable_caching:
        if cag_metadata is None:
            from ..middleware.precedent_cag_loader import load_precedent_cag
            cag_metadata = load_precedent_cag()
            if cag_metadata is None:
                logger.warning("CAG ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. ìºì‹±ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
                cag_metadata = ""
        _cag_metadata = cag_metadata
        logger.info(f"âœ… CAG ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ (í¬ê¸°: {len(cag_metadata)} ë°”ì´íŠ¸, TTL: {ttl_minutes}ë¶„)")
    else:
        # ìºì‹± ë¹„í™œì„±í™” ì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œí•˜ì§€ ì•ŠìŒ
        logger.info("â­ï¸ CAG ë©”íƒ€ë°ì´í„° ë¡œë“œ ìŠ¤í‚µ (ìºì‹± ë¹„í™œì„±í™”)")
        cag_metadata = None
        _cag_metadata = None

    _gemini_client_instance = GeminiClient(
        cag_metadata=cag_metadata,
        enable_cag_caching=enable_caching,
        cag_cache_ttl_minutes=ttl_minutes
    )

    logger.info("âœ… Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    return _gemini_client_instance


def get_gemini_client() -> GeminiClient:
    """
    Gemini í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    CAG ë©”íƒ€ë°ì´í„°ëŠ” ì•± ì‹œì‘ì‹œ initialize_gemini_client_with_cag()ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.
    ì•”ì‹œì  ìºì‹± í™œì„±í™” ì—¬ë¶€ëŠ” ENABLE_CAG_CACHING í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´í•©ë‹ˆë‹¤.
    """
    global _gemini_client_instance

    if _gemini_client_instance is None:
        # ë°±ì—…: ì§ì ‘ ì´ˆê¸°í™” (ê¶Œì¥í•˜ì§€ ì•ŠìŒ, app lifespanì—ì„œ initialize_gemini_client_with_cag() í˜¸ì¶œ í•„ìš”)
        from ..config import get_settings
        settings = get_settings()
        logger.warning(
            "âš ï¸  Gemini í´ë¼ì´ì–¸íŠ¸ë¥¼ ì§ì ‘ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. "
            "app.pyì˜ lifespanì—ì„œ initialize_gemini_client_with_cag()ì„ í˜¸ì¶œí•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤."
        )
        _gemini_client_instance = GeminiClient(
            cag_metadata=None,
            enable_cag_caching=settings.enable_cag_caching,
            cag_cache_ttl_minutes=settings.cag_cache_ttl_minutes
        )

    return _gemini_client_instance
