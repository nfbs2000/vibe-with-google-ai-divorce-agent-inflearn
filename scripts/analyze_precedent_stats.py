#!/usr/bin/env python3
"""
[í†µê³„ ë¶„ì„ ì‹¤í—˜] Gemini File Searchë¥¼ ì´ìš©í•œ íŒë¡€ íŠ¸ë Œë“œ ë¶„ì„

ë‹¨ìˆœ ê²€ìƒ‰ì„ ë„˜ì–´, LLMì—ê²Œ "ë°ì´í„° ë¶„ì„ê°€" ì—­í• ì„ ë§¡ê¸°ëŠ” ì‹¤í—˜ì…ë‹ˆë‹¤.
ì •í•´ì§„ í‚¤ì›Œë“œ(ì´í˜¼ ì‚¬ìœ  ë“±)ì— ëŒ€í•´ File Searchë¥¼ ìˆ˜í–‰í•˜ê³ , 
ê²€ìƒ‰ëœ ê·¼ê±° ë¬¸ì„œ(Citation)ì˜ ê°œìˆ˜ë¥¼ ì„¸ì–´ 'ë¹ˆë„ìˆ˜ í†µê³„'ë¥¼ ë½‘ì•„ëƒ…ë‹ˆë‹¤.

ëª©ì :
1. RAG ì‹œìŠ¤í…œì´ ë‹¨ìˆœ Q&Aë¿ë§Œ ì•„ë‹ˆë¼ ê±°ì‹œì ì¸ í†µê³„ ë„ì¶œì—ë„ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦
2. "ê²½ì œì  ê°ˆë“±ì´ ì´í˜¼ ì‚¬ìœ ì¸ íŒë¡€ê°€ ëª‡ ê±´ì¸ê°€?" ê°™ì€ ì§ˆë¬¸ í•´ê²°

ì‘ë™ ì›ë¦¬:
- í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ìˆœíšŒ -> File Search ì¿¼ë¦¬ -> ì‘ë‹µì˜ Grounding Metadata ë¶„ì„ -> ê³ ìœ í•œ ë¬¸ì„œ ê°œìˆ˜ ì¹´ìš´íŒ…
"""
import os
import json
import re
from google import genai
from google.genai import types

def load_env_file(filepath=".env"):
    """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
    try:
        with open(filepath, "r") as f:
            for line in f:
                if "=" in line and not line.strip().startswith("#"):
                    key, value = line.strip().split("=", 1)
                    os.environ[key.strip()] = value.strip()
    except FileNotFoundError:
        pass

load_env_file()
API_KEY = os.getenv("GOOGLE_API_KEY")
STORE_NAME_ID = os.getenv("PRECEDENT_FILE_SEARCH_STORE_NAME")

if not API_KEY or not STORE_NAME_ID:
    print("âŒ API Key or Store Name missing in .env")
    exit(1)

client = genai.Client(api_key=API_KEY)

# ë¶„ì„í•  í‚¤ì›Œë“œ ëª©ë¡ (ì´í˜¼ ì†Œì†¡ì˜ ì£¼ìš” ìŸì ë“¤)
STAT_KEYWORDS = [
    "ë¶€ì •í–‰ìœ„", 
    "í­í–‰ ë˜ëŠ” í•™ëŒ€", 
    "ê²½ì œì  ê°ˆë“±", 
    "ì„±ê²© ì°¨ì´", 
    "ê°€ì¶œ ë˜ëŠ” ìœ ê¸°",
    "ìë…€ ì–‘ìœ¡"
]

def analyze_precedents():
    results = {}
    total_keywords = len(STAT_KEYWORDS)
    
    print(f"ğŸ“Š íŒë¡€ ë°ì´í„° ë¶„ì„ ì‹œì‘ (ëŒ€ìƒ í‚¤ì›Œë“œ: {total_keywords}ê°œ)...")
    
    for idx, keyword in enumerate(STAT_KEYWORDS):
        print(f"[{idx+1}/{total_keywords}] '{keyword}' í‚¤ì›Œë“œ ë¶„ì„ ì¤‘ ... ", end="", flush=True)
        
        try:
            # LLMì—ê²Œ í†µê³„ ìƒì„±ì„ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸
            # ë‹¨ìˆœ ê²€ìƒ‰ì´ ì•„ë‹ˆë¼ "ê°œìˆ˜ë¥¼ ì„¸ì–´ì¤˜(count logic)"ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.
            query = f"íŒë¡€ ì €ì¥ì†Œ ì „ì²´ì—ì„œ '{keyword}'ì™€ ê´€ë ¨ëœ ì´ìŠˆê°€ í•µì‹¬ ìŸì ì´ê±°ë‚˜ ì´í˜¼ ì‚¬ìœ ë¡œ ì–¸ê¸‰ëœ íŒë¡€ë“¤ì„ ëª¨ë‘ ì°¾ì•„ì„œ ê·¸ ê°œìˆ˜ë¥¼ ì„¸ì–´ì¤˜. ê·¸ë¦¬ê³  í•´ë‹¹ë˜ëŠ” íŒë¡€ ë²ˆí˜¸ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë‚˜ì—´í•´ì¤˜."
            
            response = client.models.generate_content(
                model="gemini-2.5-flash", # ë¶„ì„ ì‘ì—…ì—ëŠ” ë¹ ë¥´ê³  ì €ë ´í•œ Flash ëª¨ë¸ ì‚¬ìš©
                contents=query,
                config=types.GenerateContentConfig(
                    tools=[
                        types.Tool(
                            file_search=types.FileSearch(
                                file_search_store_names=[STORE_NAME_ID]
                            )
                        )
                    ]
                )
            )
            
            answer_text = response.text if response.text else ""
            
            # [í†µê³„ ì¶”ì¶œ ë¡œì§]
            # LLMì˜ ë§(Text)ì„ ë¯¿ëŠ” ëŒ€ì‹ , ì‹¤ì œë¡œ ê·¼ê±°ë¡œ ì œì‹œëœ ë¬¸ì„œ(Citation)ì˜ ê°œìˆ˜ë¥¼ ì…‰ë‹ˆë‹¤.
            # ì´ê²ƒì´ ë” ì •í™•í•œ 'Proxy Metric'ì´ ë©ë‹ˆë‹¤.
            
            citation_count = 0
            citations = []
            if response.candidates and response.candidates[0].grounding_metadata:
                meta = response.candidates[0].grounding_metadata
                if meta.grounding_chunks:
                    # URI/Title ê¸°ë°˜ ì¤‘ë³µ ì œê±° (í•œ ë¬¸ì„œê°€ ì—¬ëŸ¬ ë²ˆ ì¸ìš©ë  ìˆ˜ ìˆìŒ)
                    seen = set()
                    for chunk in meta.grounding_chunks:
                        if chunk.retrieved_context:
                            title = chunk.retrieved_context.title or "Untitled"
                            if title not in seen:
                                citations.append(title)
                                seen.add(title)
            
            citation_count = len(citations)
            
            results[keyword] = {
                "count_proxy": citation_count, # ê·¼ê±° ë¬¸ì„œ ìˆ˜
                "found_cases": citations,      # ë°œê²¬ëœ íŒë¡€ ëª©ë¡
                "summary": answer_text[:200] + "..." # ë‹µë³€ ìš”ì•½
            }
            print(f"âœ… {citation_count}ê±´ì˜ ê´€ë ¨ íŒë¡€ ë°œê²¬.")
            
        except Exception as e:
            print(f"âŒ Error: {e}")
            results[keyword] = {"error": str(e)}

    # ê²°ê³¼ JSON ì €ì¥
    output_path = "data/precedent_stats.json"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
        
    print(f"\nğŸ’¾ í†µê³„ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")

if __name__ == "__main__":
    analyze_precedents()
