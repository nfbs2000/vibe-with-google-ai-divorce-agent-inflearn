#!/usr/bin/env python3
import os
import json
import logging
import time
from google import genai
from google.genai import types
from pathlib import Path
from google.cloud import bigquery
from typing import List, Dict, Any

# ë¡œê¹… ì„¤ì • (Logging Setup)
# ì‹œê°„ê³¼ ë¡œê·¸ ë ˆë²¨ì„ í¬í•¨í•˜ì—¬ ì‹¤í–‰ ìƒíƒœë¥¼ ëª…í™•íˆ ì¶”ì í•©ë‹ˆë‹¤.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_env_file(filepath=".env"):
    """
    .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    API í‚¤ë‚˜ í”„ë¡œì íŠ¸ ì„¤ì • ë“± ë¯¼ê°í•œ ì •ë³´ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    try:
        with open(filepath, "r") as f:
            for line in f:
                if "=" in line and not line.strip().startswith("#"):
                    key, value = line.strip().split("=", 1)
                    os.environ[key.strip()] = value.strip()
    except FileNotFoundError:
        pass

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_env_file()

class BatchIndexer:
    """
    íŒë¡€ ë°ì´í„°ë¥¼ BigQueryì— ì¼ê´„ ì ì¬(Batch Indexing)í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. BigQuery í…Œì´ë¸” ì´ˆê¸°í™” (DROP & CREATE)
    2. ë¡œì»¬ JSON íŒŒì¼ ì½ê¸° ë° ì „ì²˜ë¦¬
    3. Gemini APIë¥¼ ì´ìš©í•œ ì„ë² ë”© ìƒì„± (Rate Limit ê³ ë ¤í•œ ìˆœì°¨ ì²˜ë¦¬ + Sleep)
    4. BigQueryì— ë°ì´í„° ì¼ê´„ ì‚½ì… (Insert Rows)
    """
    def __init__(self):
        # í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ í”„ë¡œì íŠ¸ ì„¤ì •
        self.project_id = os.getenv("GCP_PROJECT_ID") or os.getenv("GOOGLE_CLOUD_PROJECT") or "pio-test-36cf5"
        self.dataset_id = "divorce_analytics"
        self.table_id = "precedent_cases"
        
        # API í‚¤ í™•ì¸
        self.api_key = os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            logger.error("âŒ GOOGLE_API_KEY is missing!")
            raise ValueError("GOOGLE_API_KEY is missing")
            
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.genai_client = genai.Client(api_key=self.api_key)
        self.bq_client = bigquery.Client(project=self.project_id)
        
    def reset_table(self):
        """
        ê¸°ì¡´ í…Œì´ë¸”ì„ ì‚­ì œí•˜ê³  ìŠ¤í‚¤ë§ˆì— ë§ì¶° ì¬ìƒì„±í•©ë‹ˆë‹¤.
        ì´ëŠ” ì¤‘ë³µ ë°ì´í„°ë¥¼ ë°©ì§€í•˜ê³  í•­ìƒ ê¹¨ë—í•œ ìƒíƒœ(Clean Slate)ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
        """
        table_ref = f"{self.project_id}.{self.dataset_id}.{self.table_id}"
        logger.info(f"ğŸ—‘ï¸  Dropping table {table_ref}...")
        try:
            # í…Œì´ë¸”ì´ ì—†ì–´ë„ ì—ëŸ¬ê°€ ë‚˜ì§€ ì•Šë„ë¡ not_found_ok=True ì„¤ì •
            self.bq_client.delete_table(table_ref, not_found_ok=True)
            logger.info("âœ… Table dropped.")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to drop table: {e}")

        # ìŠ¤í‚¤ë§ˆ ì •ì˜: ë²¡í„° ì»¬ëŸ¼(full_text_embedding)ì€ REPEATED FLOAT64 ëª¨ë“œë¡œ ì„¤ì •
        schema = [
            bigquery.SchemaField("case_id", "STRING"),
            bigquery.SchemaField("case_number", "STRING"),
            bigquery.SchemaField("case_type", "STRING"),
            bigquery.SchemaField("alimony_amount", "INT64"),
            bigquery.SchemaField("alimony_reason", "STRING"),
            bigquery.SchemaField("property_ratio_plaintiff", "FLOAT64"),
            bigquery.SchemaField("marriage_duration_years", "INT64"),
            bigquery.SchemaField("fault_type", "STRING"),
            bigquery.SchemaField("summary", "STRING"),
            bigquery.SchemaField("has_children", "BOOLEAN"),
            bigquery.SchemaField("full_text_embedding", "FLOAT64", mode="REPEATED")
        ]
        
        table = bigquery.Table(table_ref, schema=schema)
        # í´ëŸ¬ìŠ¤í„°ë§ í•„ë“œ ì„¤ì • (ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™”)
        table.clustering_fields = ["fault_type"]
        self.bq_client.create_table(table)
        logger.info(f"âœ… Table recreated: {table_ref}")

    def run(self, json_dir: str):
        """
        ì§€ì •ëœ ë””ë ‰í† ë¦¬ì˜ JSON íŒŒì¼ë“¤ì„ ì½ì–´ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  BigQueryì— ì ì¬í•©ë‹ˆë‹¤.
        """
        # 1. í…Œì´ë¸” ë¦¬ì…‹
        self.reset_table()
        
        p = Path(json_dir)
        json_files = sorted(p.glob("**/*.json"))
        
        if not json_files:
            logger.warning("No JSON files found!")
            return

        logger.info(f"ğŸ“‚ Found {len(json_files)} files. Preparing batch...")
        
        # ë°ì´í„° ì¤€ë¹„
        rows_map = {} # ì¸ë±ìŠ¤ë³„ ë©”íƒ€ë°ì´í„° ë§¤í•‘
        texts_to_embed = []
        
        valid_files = [] 
        
        # 2. íŒŒì¼ ì½ê¸° ë° í…ìŠ¤íŠ¸ êµ¬ì„±
        for idx, json_file in enumerate(json_files):
            try:
                with open(json_file, 'r') as f:
                    metadata = json.load(f)
                
                # ë©”íƒ€ë°ì´í„° Validation
                case_id = metadata.get('case_id', json_file.stem)
                
                # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ êµ¬ì„±: íŒë¡€ì˜ í•µì‹¬ ì •ë³´ë“¤ì„ ì¡°í•©í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
                summary = metadata.get('key_summary', '')
                reason = metadata.get('alimony_reason', '')
                fault = metadata.get('fault_type', 'Unknown')
                
                text = f"Case: {metadata.get('filename', 'Unknown')}\nSummary: {summary}\nReason: {reason}\nFault: {fault}"
                
                if not text.strip():
                    logger.warning(f"âš ï¸ Empty text for {json_file.name}, skipping.")
                    continue
                    
                rows_map[len(texts_to_embed)] = metadata # ì¸ë±ìŠ¤ë¡œ ì €ì¥
                texts_to_embed.append(text)
                valid_files.append(json_file.name)
                
            except Exception as e:
                logger.error(f"âŒ Error reading {json_file}: {e}")

        logger.info(f"ğŸ¤– Starting sequential embedding for {len(texts_to_embed)} items (with delay)...")
        
        if not texts_to_embed:
            logger.error("No valid texts to embed.")
            return

        rows_to_insert = []
        
        # 3. ì„ë² ë”© ìƒì„± (ìˆœì°¨ ì²˜ë¦¬)
        # ì¤‘ìš”: API Rate Limit(RPM)ì„ í”¼í•˜ê¸° ìœ„í•´ Batch call ëŒ€ì‹  Loop + Sleep ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        for i, text in enumerate(texts_to_embed):
            try:
                # ê°œë³„ ì„ë² ë”© ìš”ì²­
                response = self.genai_client.models.embed_content(
                    model="text-embedding-004",
                    contents=text
                )
                
                # ë²¡í„° ì¶”ì¶œ
                if hasattr(response, 'embeddings') and response.embeddings:
                    vector = response.embeddings[0].values
                else:
                    # Some SDK versions return distinct structure?
                    logger.warning(f"âš ï¸ Unexpected response structure for {valid_files[i]}")
                    continue

                if not vector:
                     logger.warning(f"âš ï¸ Empty vector for {valid_files[i]}")
                     continue

                metadata = rows_map[i]
                
                # BigQuery ì ì¬ìš© Row ìƒì„±
                row = {
                    "case_id": metadata.get('case_id', 'Unknown'),
                    "case_number": metadata.get('filename', 'Unknown'),
                    "case_type": metadata.get('case_type', 'Unknown'),
                    "alimony_amount": int(metadata.get('alimony_final_amount', 0)),
                    "alimony_reason": metadata.get('alimony_reason', ''),
                    "property_ratio_plaintiff": float(metadata.get('property_ratio_plaintiff', 0.0)),
                    "marriage_duration_years": int(metadata.get('marriage_duration_years', -1)),
                    "fault_type": metadata.get('fault_type', 'Unknown'),
                    "summary": metadata.get('key_summary', ''),
                    "has_children": metadata.get('has_children', None),
                    "full_text_embedding": vector
                }
                rows_to_insert.append(row)
                logger.info(f"âœ… Embedded {i+1}/{len(texts_to_embed)}: {valid_files[i]}")
                
                # [Rate Limit ë°©ì–´] 2ì´ˆ ëŒ€ê¸°
                time.sleep(2) 

            except Exception as e:
                logger.error(f"ğŸ”¥ Failed to embed {valid_files[i]}: {e}")
        
        # 4. BigQuery ì¼ê´„ ì ì¬
        # ëª¨ë“  ì„ë² ë”©ì´ ì¤€ë¹„ë˜ë©´ í•œ ë²ˆì— Insert í•©ë‹ˆë‹¤.
        if rows_to_insert:
            logger.info(f"ğŸš€ Inserting {len(rows_to_insert)} rows to BigQuery...")
            try:
                errors = self.bq_client.insert_rows_json(f"{self.project_id}.{self.dataset_id}.{self.table_id}", rows_to_insert)
                if not errors:
                    logger.info("ğŸ‰ SUCCESS! All data indexed.")
                else:
                    logger.error(f"âš ï¸ Insert errors: {errors}")
            except Exception as e:
                # Fallback for streaming buffer issues (though we dropped table so should be fine)
                 logger.error(f"âš ï¸ Insert failed: {e}")

if __name__ == "__main__":
    indexer = BatchIndexer()
    # ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ì´ ìœ„ì¹˜í•œ ê²½ë¡œë¥¼ ì§€ì •í•˜ì—¬ ì‹¤í–‰
    indexer.run("data/court_cases/metadata_json")
